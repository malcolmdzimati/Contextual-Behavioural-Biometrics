{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Librabies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Librabies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Transactional Data and Non statistical exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data and renaming columns as well as dropping uneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactional_data = pd.read_csv(\"../DataSets/TransactionalData/Synthetic_Financial_datasets_log.csv\")\n",
    "\n",
    "transactional_data = transactional_data[transactional_data['type'] != 'CASH_IN']\n",
    "\n",
    "transactional_data['nameOrig'] = transactional_data['nameOrig'].str[:4]\n",
    "transactional_data['nameDest'] = transactional_data['nameDest'].apply(lambda x: x[:3] if x.startswith('M') else x)\n",
    "\n",
    "print(\"Fraud: \" + str(len(transactional_data[transactional_data['isFraud'] == 1])) + \"; Not Fraud: \" + str(len(transactional_data[transactional_data['isFraud'] == 0])) + \"; Flagged Fraud: \" + str(len(transactional_data[transactional_data['isFlaggedFraud'] == 1])))\n",
    "\n",
    "user_transaction_count = transactional_data.groupby('nameOrig').size().reset_index(name='transaction_count')\n",
    "\n",
    "transactional_data.loc[transactional_data['nameDest'].str.startswith('C'), 'nameDest'] = 'user'\n",
    "\n",
    "unique_users_count = transactional_data['nameOrig'].nunique()\n",
    "unique_merch_count = transactional_data['nameDest'].nunique()\n",
    "\n",
    "print(f'Total distinct users: {unique_users_count}')\n",
    "print(f'Total distinct merch: {unique_merch_count}')\n",
    "\n",
    "\n",
    "# Sort by the number of transactions in descending order\n",
    "sorted_users = user_transaction_count.sort_values(by='transaction_count', ascending=False)\n",
    "\n",
    "top_10_users = sorted_users.head(10)\n",
    "print(top_10_users)\n",
    "\n",
    "\n",
    "#del transactional_data['newbalanceDest']\n",
    "\n",
    "\n",
    "\n",
    "# Assuming your simulation starts at a specific date and time\n",
    "start_datetime = '2023-01-01 00:00:00'  # Replace with your actual start date and time\n",
    "steps = transactional_data['step'].max()  # Total simulation steps (hours)\n",
    "\n",
    "# Create a date range for the number of steps, incrementing by 1 hour\n",
    "date_range = pd.date_range(start=start_datetime, periods=steps, freq='1d')\n",
    "\n",
    "# Total number of records\n",
    "total_records = len(transactional_data)\n",
    "\n",
    "\n",
    "# Assign datetime based on the step\n",
    "transactional_data['transaction_date'] = transactional_data['step'].apply(lambda x: date_range[x-1])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(transactional_data['type'].unique())\n",
    "\n",
    "print(len(transactional_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insure all the data is in correct format for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "chars_to_remove = [' ', ',']\n",
    "\n",
    "for char in chars_to_remove:\n",
    "    transactional_data['amount'] = transactional_data['amount'].replace(char, '', regex=True)\n",
    "    transactional_data['oldbalanceOrg'] = transactional_data['oldbalanceOrg'].replace(char, '', regex=True)\n",
    "    transactional_data['newbalanceOrig'] = transactional_data['newbalanceOrig'].replace(char, '', regex=True)\n",
    "\n",
    "transactional_data['amount'] = transactional_data['amount'].astype(np.float32)\n",
    "transactional_data['oldbalanceOrg'] = transactional_data['oldbalanceOrg'].astype(np.float32)\n",
    "transactional_data['newbalanceOrig'] = transactional_data['newbalanceOrig'].astype(np.float32)\n",
    "\n",
    "#sorted_data = data = transactional_data.sort_values(by=['nameOrig', 'step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transactional_data[transactional_data['isFraud'] == 0]\n",
    "\n",
    "#df = transactional_data\n",
    "# Calculate 10% of the original size\n",
    "sample_size = int(len(df) * 0.05)\n",
    "\n",
    "df.sort_values('step')\n",
    "\n",
    "df = df.head(sample_size)\n",
    "\n",
    "print(len(df[df['isFraud'] == 1]))\n",
    "print(len(df))\n",
    "\n",
    "df_later = transactional_data[sample_size:sample_size*4]\n",
    "\n",
    "print(len(df_later[df_later['isFraud'] == 1]))\n",
    "print(len(df_later))\n",
    "\n",
    "print(len(df_later[df_later['isFlaggedFraud'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['transaction_date', 'step'], inplace=True)\n",
    "df_later.sort_values(by=['transaction_date', 'step'], inplace=True)\n",
    "users = df_later[\"nameOrig\"].unique()\n",
    "user_data =  df_later[df_later[\"nameOrig\"] == users[0]][:1000]\n",
    "user_data_later =  df_later[df_later[\"nameOrig\"] == users[0]][1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flagged = transactional_data[transactional_data['isFraud']==1]['isFlaggedFraud'].value_counts()\n",
    "\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(flagged.index, flagged.values, color='skyblue')\n",
    "\n",
    "plt.xticks([0, 1], ['NotFlagged', 'Flagged'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Fraudulent Transaction (0 = No, 1 = Yes)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.title('Number of Flagged Fraudulent Transactions in Data')\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "def create_data_for_model(data):\n",
    "    # Step 4: Convert `transaction_date` to datetime and extract day, month, and day of week as features\n",
    "    data.loc[:, 'transaction_date'] = pd.to_datetime(data['transaction_date'], format='%d-%b-%y')\n",
    "    data.loc[:, 'day_of_week'] = data['transaction_date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    data.loc[:, 'month'] = data['transaction_date'].dt.month\n",
    "\n",
    "    # Step 5: Encode categorical variables (transaction_type and merchant)\n",
    "    le_merchant = LabelEncoder()\n",
    "    data.loc[:, 'nameDest'] = le_merchant.fit_transform(data['nameDest'].values)\n",
    "\n",
    "    le_transaction_type = LabelEncoder()\n",
    "    data.loc[:, 'type'] = le_transaction_type.fit_transform(data['type'].values)\n",
    "\n",
    "    print(data[['amount', 'oldbalanceOrg', 'newbalanceOrig']])\n",
    "    standard_scaler = StandardScaler()\n",
    "    data[['amount', 'oldbalanceOrg', 'newbalanceOrig']] = standard_scaler.fit_transform(data[['amount', 'oldbalanceOrg', 'newbalanceOrig']])\n",
    "\n",
    "    print(data[['amount', 'oldbalanceOrg', 'newbalanceOrig']])\n",
    "    print(data[['amount', 'oldbalanceOrg', 'newbalanceOrig']].dtypes)\n",
    "    print(data[['amount', 'oldbalanceOrg', 'newbalanceOrig']].isna().sum())\n",
    "\n",
    "    return data\n",
    "\n",
    "df = create_data_for_model(df)\n",
    "\n",
    "# Step 3: Prepare data for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Select features for LSTM\n",
    "features = df[['amount', 'nameDest', 'oldbalanceOrg', 'newbalanceOrig', 'type', 'step', 'day_of_week', 'month']].values\n",
    "SEQ_LENGTH = 12\n",
    "X = create_sequences(features, SEQ_LENGTH)\n",
    "\n",
    "# Verify the shapes of the output\n",
    "print(\"Shape of X (sequences):\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "def create_lstm_autoencoder(timesteps, n_features, latent_dim=64):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(timesteps, n_features))\n",
    "\n",
    "    # Encoder with optimized architecture\n",
    "    encoded = LSTM(256, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001))(inputs)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "    encoded = LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001))(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.3)(encoded)\n",
    "    encoded = LSTM(64, activation='tanh', return_sequences=False, kernel_regularizer=l2(0.001))(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "\n",
    "    # Latent space representation\n",
    "    latent = Dense(latent_dim, activation='tanh')(encoded)\n",
    "\n",
    "    # Decoder with optimized architecture\n",
    "    decoded = RepeatVector(timesteps)(latent)\n",
    "    decoded = LSTM(64, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001))(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.3)(decoded)\n",
    "    decoded = LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001))(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.3)(decoded)\n",
    "    decoded = LSTM(256, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.001))(decoded)\n",
    "\n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(n_features))(decoded)\n",
    "\n",
    "    # Full autoencoder model\n",
    "    autoencoder = Model(inputs, output)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM autoencoder with chosen parameters\n",
    "neuralnetwork = LSTMAutoencoder(\n",
    "    hidden_units=hidden_units,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ").to(device=device)\n",
    "\n",
    "# Use reconstruction loss for an autoencoder\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for X_train_batch in train_loader:\n",
    "    X_train_batch = X_train_batch.to(device=device)\n",
    "    optimizer.zero_grad()\n",
    "    y_batch_prediction = neuralnetwork(X_train_batch)\n",
    "    loss = criterion(y_batch_prediction, X_train_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and compile the model\n",
    "latent_dim = 64\n",
    "global_autoencoder = create_lstm_autoencoder(SEQ_LENGTH, X.shape[2], latent_dim)\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9\n",
    ")\n",
    "\n",
    "# Compile with Adam and learning rate scheduler\n",
    "global_autoencoder.compile(optimizer=Adam(learning_rate=lr_schedule), loss='mae')\n",
    "\n",
    "# Optionally add Early Stopping for training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# If using a Pandas DataFrame, convert the DataFrame to a NumPy array and ensure numeric types\n",
    "X = X.astype(np.float32)\n",
    "\n",
    "# Assuming 'global_transaction_data' is the dataset for general training (preprocesse\n",
    "# Train the global model\n",
    "histogram_global = global_autoencoder.fit(X, X, epochs=30, batch_size=16, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(histogram_global.history['loss'], label='Training Loss')\n",
    "plt.plot(histogram_global.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed \n",
    "import joblib \n",
    "  \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(global_autoencoder, 'contextual_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "df_later = create_data_for_model(df_later)\n",
    "user_data = create_data_for_model(user_data)\n",
    "user_data_later = create_data_for_model(user_data_later)\n",
    "\n",
    "# Path to the pickle file\n",
    "pickle_file = 'contextual_model.pkl'\n",
    "\n",
    "# Load the model from the pickle file\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    global_autoencoder = pickle.load(file)\n",
    "\n",
    "# Now the model is loaded and you can use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to detect anomalies with multiple thresholds\n",
    "def detect_anomalies_with_confidence(data, model, thresholds):\n",
    "    data = data.astype(np.float32)\n",
    "    # Get reconstructed data\n",
    "    reconstructed = model.predict(data)\n",
    "    # Calculate reconstruction errors\n",
    "    reconstruction_errors = np.mean(np.square(data - reconstructed), axis=(1, 2))\n",
    "    \n",
    "     # Initialize an empty list to store anomaly results\n",
    "    anomaly_results = []\n",
    "\n",
    "    print(reconstruction_errors)\n",
    "    \n",
    "    # Compare reconstruction errors with thresholds to find sureness level\n",
    "    # Compare reconstruction errors with thresholds to classify anomaly level\n",
    "    for error in reconstruction_errors:  # 'error' is now a scalar\n",
    "        if error > thresholds['high']:\n",
    "            anomaly_results.append('high')\n",
    "        elif error > thresholds['medium']:\n",
    "            anomaly_results.append('medium')\n",
    "        elif error > thresholds['low']:\n",
    "            anomaly_results.append('low')\n",
    "        else:\n",
    "            anomaly_results.append('none')\n",
    "    \n",
    "    return anomaly_results, reconstruction_errors\n",
    "\n",
    "# Define different thresholds for different confidence levels\n",
    "# Adjust these based on your validation set or domain knowledge\n",
    "thresholds = {\n",
    "    'low': 55,     # High confidence (only very anomalous points)\n",
    "    'medium': 50,  # Medium confidence\n",
    "    'high': 53     # Low confidence (more points classified as anomalies)\n",
    "}\n",
    "\n",
    "anomaly_levels, reconstruction_errors = detect_anomalies_with_confidence(X, global_autoencoder, thresholds)\n",
    "\n",
    "# View the results\n",
    "print(anomaly_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to detect anomalies with multiple thresholds\n",
    "def detect_anomalies_with_confidence(data, model, thresholds):        \n",
    "    features_later = data[['amount', 'nameDest', 'oldbalanceOrg', 'newbalanceOrig', 'type', 'step', 'day_of_week', 'month']].values\n",
    "    SEQ_LENGTH = 12\n",
    "    X = create_sequences(features_later, SEQ_LENGTH)\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    # Get reconstructed data\n",
    "    reconstructed = model.predict(X)\n",
    "    # Calculate reconstruction errors\n",
    "    reconstruction_errors = np.mean(np.square(X - reconstructed), axis=(1, 2))\n",
    "    \n",
    "     # Initialize an empty list to store anomaly results\n",
    "    anomaly_results = []\n",
    "    \n",
    "    # Compare reconstruction errors with thresholds to find sureness level\n",
    "    for error in reconstruction_errors:\n",
    "        if error > thresholds['high']:\n",
    "            anomaly_results.append('high')\n",
    "        elif error > thresholds['medium']:\n",
    "            anomaly_results.append('medium')\n",
    "        elif error > thresholds['low']:\n",
    "            anomaly_results.append('low')\n",
    "        else:\n",
    "            anomaly_results.append('none')  # No anomaly detected\n",
    "    \n",
    "    return anomaly_results, reconstruction_errors\n",
    "\n",
    "# Define different thresholds for different confidence levels\n",
    "# Adjust these based on your validation set or domain knowledge\n",
    "thresholds = {\n",
    "    'low': 2400,     # High confidence (only very anomalous points)\n",
    "    'medium': 2500,  # Medium confidence\n",
    "    'high': 2600    # Low confidence (more points classified as anomalies)\n",
    "}\n",
    "\n",
    "anomaly_levels, reconstruction_errors = detect_anomalies_with_confidence(df_later, global_autoencoder, thresholds)\n",
    "\n",
    "# Handle anomaly_levels length mismatch\n",
    "if len(anomaly_levels) < len(df_later):\n",
    "    anomaly_levels = np.concatenate([anomaly_levels, np.full(len(df_later) - len(anomaly_levels), np.nan)])\n",
    "elif len(anomaly_levels) > len(df_later):\n",
    "    anomaly_levels = anomaly_levels[:len(df_later)]\n",
    "\n",
    "# Handle reconstruction_errors length mismatch\n",
    "reconstruction_errors = np.concatenate([reconstruction_errors, np.full(len(df_later) - len(reconstruction_errors), np.nan)])\n",
    "\n",
    "# Assign the corrected values to the DataFrame\n",
    "df_later['anomaly_level'] = anomaly_levels\n",
    "df_later['reconstruction_error'] = reconstruction_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Now, we can show the fraud label and anomaly level:\n",
    "df_output = df_later[['isFraud', 'anomaly_level', 'reconstruction_error']]\n",
    "\n",
    "true_negative = len(df_output[(df_output['anomaly_level'] == 'low') | (df_output['anomaly_level'] == 'none') & (df_output['isFraud'] == 0)]) \n",
    "\n",
    "true_positive = len(df_output[(df_output['anomaly_level'] != 'none') & (df_output['isFraud'] == 1)])\n",
    "\n",
    "false_negative = len(df_output[(df_output['anomaly_level'] == 'none') & (df_output['isFraud'] == 1)])\n",
    "\n",
    "false_positive = len(df_output[(df_output['anomaly_level'] != 'low') & (df_output['anomaly_level'] != 'none') & (df_output['anomaly_level'] != 'high')  & (df_output['isFraud'] == 0)])\n",
    "\n",
    "acc = ((true_negative+true_positive)/len(df_output)) \n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(true_positive)\n",
    "print(false_positive)\n",
    "print(true_negative)\n",
    "print(len(df_output[df_output['isFraud'] == 1]))\n",
    "print(len(df_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploration of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fraud_detection_model(autoencoder):\n",
    "    # Freeze all encoder layers\n",
    "    for layer in autoencoder.layers[:5]:  # Assuming the first 5 layers are the encoder layers\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Extract the encoder output\n",
    "    encoder_output = autoencoder.layers[-7].output  # Adjust index if needed based on the model structure\n",
    "\n",
    "    # Define a new model combining encoder and classifier\n",
    "    fraud_detector = Model(inputs=autoencoder.input, outputs=encoder_output)\n",
    "\n",
    "    # Compile the model with a binary cross-entropy loss\n",
    "    fraud_detector.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    return fraud_detector\n",
    "\n",
    "# Create the fraud detection model using the pre-trained autoencoder\n",
    "fraud_detector = build_fraud_detection_model(global_autoencoder)\n",
    "\n",
    "# For now, you can use the valid transactions to simulate training\n",
    "# Example:\n",
    "fraud_detector.fit(user_data, user_data, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_levels, reconstruction_errors = detect_anomalies_with_confidence(user_data_later, fraud_detector, thresholds)\n",
    "\n",
    "# Handle anomaly_levels length mismatch\n",
    "if len(anomaly_levels) < len(df_later):\n",
    "    anomaly_levels = np.concatenate([anomaly_levels, np.full(len(df_later) - len(anomaly_levels), np.nan)])\n",
    "elif len(anomaly_levels) > len(df_later):\n",
    "    anomaly_levels = anomaly_levels[:len(df_later)]\n",
    "\n",
    "# Handle reconstruction_errors length mismatch\n",
    "reconstruction_errors = np.concatenate([reconstruction_errors, np.full(len(df_later) - len(reconstruction_errors), np.nan)])\n",
    "\n",
    "# Assign the corrected values to the DataFrame\n",
    "df_later['anomaly_level'] = anomaly_levels\n",
    "df_later['reconstruction_error'] = reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Now, we can show the fraud label and anomaly level:\n",
    "df_output = df_later[['isFraud', 'anomaly_level', 'reconstruction_error']]\n",
    "\n",
    "true_negative = len(df_output[(df_output['anomaly_level'] == 'low') | (df_output['anomaly_level'] == 'none') & (df_output['isFraud'] == 0)]) \n",
    "\n",
    "true_positive = len(df_output[(df_output['anomaly_level'] != 'none') & (df_output['isFraud'] == 1)])\n",
    "\n",
    "false_negative = len(df_output[(df_output['anomaly_level'] == 'none') & (df_output['isFraud'] == 1)])\n",
    "\n",
    "false_positive = len(df_output[(df_output['anomaly_level'] != 'low') & (df_output['anomaly_level'] != 'none') & (df_output['anomaly_level'] != 'high')  & (df_output['isFraud'] == 0)])\n",
    "\n",
    "acc = ((true_negative+true_positive)/len(df_output)) \n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(true_positive)\n",
    "print(false_positive)\n",
    "print(true_negative)\n",
    "print(len(df_output[df_output['isFraud'] == 1]))\n",
    "print(len(df_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
