{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Librabies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install spacy\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow\n",
    "%pip install user-agents\n",
    "%pip install  matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Librabies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from user_agents import parse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read User behaviors and Non statistical exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stroke = pd.read_csv(\"../DataSets/UserBehaviors/Keystroke/free-text.csv\")\n",
    "\n",
    "key_stroke = key_stroke.iloc[:, :-1]\n",
    "\n",
    "# Get the unique user IDs\n",
    "#first_10_user_ids = key_stroke['participant'].unique()[:10]\n",
    "\n",
    "# Filter the data for the first 10 unique users\n",
    "#first_10_users_data = key_stroke[key_stroke['participant'].isin(first_10_user_ids)]\n",
    "\n",
    "# Drop unneeded columns\n",
    "#first_10_users_data = first_10_users_data.drop(first_10_users_data.columns[-1], axis=1)\n",
    "\n",
    "# Display head\n",
    "key_stroke.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mouse movement && Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_info = pd.read_csv(\"../DataSets/UserBehaviors/mousedynamics/EVTRACKINFO/EVTRACKINFO.csv\", sep='\\t')\n",
    "mouse_movements = pd.read_csv(\"../DataSets/UserBehaviors/mousedynamics/EVTRACKTRACK/EVTRACKTRACK.csv\", sep='\\t')\n",
    "\n",
    "\n",
    "# Filter uneed data\n",
    "mouse_data = mouse_movements[mouse_movements['event'].str.contains('mouse', case=False, na=False)]\n",
    "\n",
    "# Display head\n",
    "# Drop unneeded columns\n",
    "del session_info['_id']\n",
    "del mouse_data['_id']\n",
    "del mouse_data['cursor']\n",
    "del session_info['documentw']\n",
    "del session_info['documenth']\n",
    "del session_info['date']\n",
    "del mouse_data['key']\n",
    "\n",
    "print(mouse_data.isnull().sum())\n",
    "print(session_info.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_browser_info_and_os(ua_string):\n",
    "    # Parse the user agent string\n",
    "    user_agent = parse(ua_string)\n",
    "\n",
    "    # Extract browser and operating system\n",
    "    browser = user_agent.browser\n",
    "    os = user_agent.os\n",
    "    device =  user_agent.device\n",
    "\n",
    "    return {\"browser\": str(browser), \"os\": str(os), \"device\": str(device)}\n",
    "\n",
    "# Apply extraction to session_info\n",
    "session_info[['browser', 'os', 'device']] = session_info['ua'].apply(lambda ua: pd.Series(extract_browser_info_and_os(ua)))\n",
    "\n",
    "session_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge mouse_data with session_info based on user and session_id\n",
    "merged_data = mouse_data.merge(session_info[['user', 'session_id', 'browser', 'os']], on=['user', 'session_id'], how='left')\n",
    "\n",
    "print(\"\\nMerged Data with Browser and OS:\")\n",
    "print(len(merged_data))\n",
    "print(len(mouse_data))\n",
    "\n",
    "# Count unique operating systems\n",
    "unique_os_count = merged_data['os'].nunique()\n",
    "print(f\"Number of unique operating systems: {unique_os_count}\")\n",
    "\n",
    "# Count unique browsers\n",
    "unique_browsers_count = merged_data['browser'].nunique()\n",
    "print(f\"Number of unique browsers: {unique_browsers_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "#le_merchant = LabelEncoder()\n",
    "#merged_data['browser'] = le_merchant.fit_transform(merged_data['browser'].values)\n",
    "\n",
    "#le_transaction_type = LabelEncoder()\n",
    "#merged_data['os'] = le_transaction_type.fit_transform(merged_data['os'].values)\n",
    "\n",
    "# Count unique operating systems\n",
    "unique_os_count = merged_data['os'].nunique()\n",
    "print(f\"Number of unique operating systems: {unique_os_count}\")\n",
    "\n",
    "# Count unique browsers\n",
    "unique_browsers_count = merged_data['browser'].nunique()\n",
    "print(f\"Number of unique browsers: {unique_browsers_count}\")\n",
    "\n",
    "print(len(merged_data[merged_data[\"user\"] == \"Alluserspreauth\"]))\n",
    "print(len(merged_data[merged_data[\"user\"] != \"Alluserspreauth\"]))\n",
    "print(len(merged_data))\n",
    "\n",
    "print(merged_data)\n",
    "\n",
    "model_mouse_data = merged_data[merged_data[\"user\"] != \"Alluserspreauth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_stroke.rename(columns={'participant': 'user'}, inplace=True)\n",
    "# Step 2: Limit to the first 11 users\n",
    "selected_users = key_stroke['user'].drop_duplicates().head(11)\n",
    "\n",
    "x = 1\n",
    "# Step 3: Rename users to Userx format\n",
    "for user in selected_users:\n",
    "    key_stroke.loc[key_stroke['user'] == user, 'user'] = \"User\" + str(x)\n",
    "    x = x+1\n",
    "\n",
    "key_stroke = key_stroke[key_stroke[\"user\"].str.contains(\"User\", case=False, na=False)]\n",
    "\n",
    "# Step 2: Create a pseudo-timestamp for keystrokes\n",
    "key_stroke['timestamp'] = pd.date_range(start='2020-02-17 09:35:49.089692', periods=60129, freq='1s')\n",
    "\n",
    "key_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge mouse and keystroke data\n",
    "# We will use a time window to align keystrokes with mouse events\n",
    "#time_window = 3  # Define a time window of 5 seconds\n",
    "\n",
    "def merge_with_time_window(mouse_data, keystroke_data, time_window):\n",
    "    # Ensure timestamps are in datetime format\n",
    "    mouse_data.loc[:, 'timestamp'] = pd.to_datetime(mouse_data['timestamp'])\n",
    "    keystroke_data.loc[:, 'timestamp'] = pd.to_datetime(keystroke_data['timestamp'])\n",
    "    \n",
    "    # Ensure time_window is an integer or float\n",
    "    if not isinstance(time_window, (int, float)):\n",
    "        raise ValueError(\"time_window must be an int or float representing seconds.\")\n",
    "    \n",
    "    merged_data = []\n",
    "    for _, mouse_row in mouse_data.iterrows():\n",
    "        # Get the user ID and timestamp for the mouse movement\n",
    "        user_id = mouse_row['user']\n",
    "        mouse_time = mouse_row['timestamp']\n",
    "        \n",
    "        # Find keystrokes within the time window\n",
    "        keystrokes_within_window = keystroke_data[\n",
    "            (keystroke_data['user'] == user_id) & \n",
    "            (keystroke_data['timestamp'] >= mouse_time - pd.Timedelta(seconds=time_window)) & \n",
    "            (keystroke_data['timestamp'] <= mouse_time + pd.Timedelta(seconds=time_window))\n",
    "        ]\n",
    "        \n",
    "        for _, keystroke_row in keystrokes_within_window.iterrows():\n",
    "            # Collect relevant information for each mouse and keystroke pair\n",
    "            merged_data.append({\n",
    "                'mouse_timestamp': mouse_time,\n",
    "                'user': user_id,\n",
    "                'xpos': mouse_row['xpos'],\n",
    "                'ypos': mouse_row['ypos'],\n",
    "                'keystroke_timestamp': keystroke_row['timestamp'],\n",
    "                'key1': keystroke_row['key1'],\n",
    "                'key2': keystroke_row['key2'],\n",
    "                'DU.key1.key1': keystroke_row['DU.key1.key1'],\n",
    "                'DD.key1.key2': keystroke_row['DD.key1.key2'],\n",
    "                'DU.key1.key2': keystroke_row['DU.key1.key2'],\n",
    "                'UD.key1.key2': keystroke_row['UD.key1.key2'],\n",
    "                'UU.key1.key2': keystroke_row['UU.key1.key2'],\n",
    "                'event': mouse_row['event'],\n",
    "                'os': mouse_row['os'],\n",
    "                'browser': mouse_row['browser']\n",
    "            })\n",
    "\n",
    "    # Convert the merged data into a DataFrame\n",
    "    merged_df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge mouse and keystroke data\n",
    "combined_data = merge_with_time_window(model_mouse_data, key_stroke, 3)\n",
    "print(combined_data)\n",
    "\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "combined_data['key_encoded_1'] = LabelEncoder().fit_transform(combined_data['key1'].fillna(''))  # Encode keys\n",
    "combined_data['key_encoded_2'] = LabelEncoder().fit_transform(combined_data['key2'].fillna(''))  # Encode keys\n",
    "combined_data['event'] = LabelEncoder().fit_transform(combined_data['event'].fillna(''))  # Encode keys\n",
    "combined_data['browser'] = LabelEncoder().fit_transform(combined_data['browser'].fillna(''))  # Encode keys\n",
    "combined_data['os'] = LabelEncoder().fit_transform(combined_data['os'].fillna(''))  # Encode keys\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "combined_data[['xpos', 'ypos', 'DU.key1.key1', 'DD.key1.key2', 'DU.key1.key2', 'UD.key1.key2', 'UU.key1.key2']] = standard_scaler.fit_transform(combined_data[['xpos', 'ypos', 'DU.key1.key1', 'DD.key1.key2', 'DU.key1.key2', 'UD.key1.key2', 'UU.key1.key2']])\n",
    "\n",
    "\n",
    "# Step 3: Prepare data for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Select features for LSTM\n",
    "features = combined_data[['xpos', 'ypos', 'key_encoded_1', 'key_encoded_2', 'event', 'os', 'browser', 'DU.key1.key1', 'DD.key1.key2', 'DU.key1.key2', 'UD.key1.key2', 'UU.key1.key2']].values\n",
    "SEQ_LENGTH = 12\n",
    "X = create_sequences(features, SEQ_LENGTH)\n",
    "\n",
    "# Verify the shapes of the output\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_lstm_autoencoder(timesteps, n_features, latent_dim=64):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(timesteps, n_features))\n",
    "\n",
    "    # Encoder with optimized architecture\n",
    "    encoded = LSTM(256, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.003))(inputs)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.4)(encoded)\n",
    "    encoded = LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.003))(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dropout(0.4)(encoded)\n",
    "    encoded = LSTM(64, activation='tanh', return_sequences=False, kernel_regularizer=l2(0.003))(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "\n",
    "    # Latent space representation\n",
    "    latent = Dense(latent_dim, activation='tanh')(encoded)\n",
    "\n",
    "    # Decoder with optimized architecture\n",
    "    decoded = RepeatVector(timesteps)(latent)\n",
    "    decoded = LSTM(64, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.003))(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.4)(decoded)\n",
    "    decoded = LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.003))(decoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dropout(0.4)(decoded)\n",
    "    decoded = LSTM(256, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.003))(decoded)\n",
    "\n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(n_features))(decoded)\n",
    "\n",
    "    # Full autoencoder model\n",
    "    autoencoder = Model(inputs, output)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and compile the model\n",
    "latent_dim = 32\n",
    "global_autoencoder = create_lstm_autoencoder(SEQ_LENGTH, X.shape[2], latent_dim)\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9\n",
    ")\n",
    "\n",
    "# Compile with Adam and learning rate scheduler\n",
    "global_autoencoder.compile(optimizer=Adam(learning_rate=lr_schedule), loss='mae')\n",
    "\n",
    "# Optionally add Early Stopping for training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Assuming 'global_transaction_data' is the dataset for general training (preprocessed)\n",
    "# Train the global model\n",
    "histogram_global = global_autoencoder.fit(X, X, epochs=180, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(histogram_global.history['loss'], label='Training Loss')\n",
    "plt.plot(histogram_global.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Learning Curve For Transactions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed \n",
    "import joblib \n",
    "  \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(global_autoencoder, 'behavioral_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Path to the pickle file\n",
    "pickle_file = 'TrainedModels/behavioral_model.pkl'\n",
    "\n",
    "# Load the model from the pickle file\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    global_autoencoder = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to detect anomalies with multiple thresholds\n",
    "def detect_anomalies_with_confidence(data, model, thresholds):\n",
    "    # Get reconstructed data\n",
    "    reconstructed = model.predict(data)\n",
    "    # Calculate reconstruction errors\n",
    "    reconstruction_errors = np.mean(np.square(data - reconstructed), axis=(1, 2))\n",
    "    \n",
    "     # Initialize an empty list to store anomaly results\n",
    "    anomaly_results = []\n",
    "\n",
    "    print(reconstruction_errors)\n",
    "    \n",
    "    # Compare reconstruction errors with thresholds to find sureness level\n",
    "    # Compare reconstruction errors with thresholds to classify anomaly level\n",
    "    for error in reconstruction_errors:  # 'error' is now a scalar\n",
    "        if error > thresholds['high']:\n",
    "            anomaly_results.append('high')\n",
    "        elif error > thresholds['medium']:\n",
    "            anomaly_results.append('medium')\n",
    "        elif error > thresholds['low']:\n",
    "            anomaly_results.append('low')\n",
    "        else:\n",
    "            anomaly_results.append('none')\n",
    "    \n",
    "    return anomaly_results, reconstruction_errors\n",
    "\n",
    "# Define different thresholds for different confidence levels\n",
    "# Adjust these based on your validation set or domain knowledge\n",
    "thresholds = {\n",
    "    'low': 6,     # High confidence (only very anomalous points)\n",
    "    'medium': 8,  # Medium confidence\n",
    "    'high': 10     # Low confidence (more points classified as anomalies)\n",
    "}\n",
    "\n",
    "anomaly_levels, reconstruction_errors = detect_anomalies_with_confidence(X, global_autoencoder, thresholds)\n",
    "\n",
    "# View the results\n",
    "print(anomaly_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fraud_detection_model(autoencoder):\n",
    "    # Freeze all encoder layers\n",
    "    for layer in autoencoder.layers[:5]:  # Assuming the first 5 layers are the encoder layers\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Extract the encoder output\n",
    "    encoder_output = autoencoder.layers[-7].output  # Adjust index if needed based on the model structure\n",
    "\n",
    "    # Define a new model combining encoder and classifier\n",
    "    fraud_detector = Model(inputs=autoencoder.input, outputs=encoder_output)\n",
    "\n",
    "    # Compile the model with a binary cross-entropy loss\n",
    "    fraud_detector.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    return fraud_detector\n",
    "\n",
    "# Create the fraud detection model using the pre-trained autoencoder\n",
    "fraud_detector = build_fraud_detection_model(global_autoencoder)\n",
    "\n",
    "# For now, you can use the valid transactions to simulate training\n",
    "# Example:\n",
    "fraud_detector.fit(X_train, X_train, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to calculate reconstruction error\n",
    "def calculate_reconstruction_error(model, data):\n",
    "    reconstructed_data = model.predict(data)\n",
    "    errors = np.mean(np.square(data - reconstructed_data), axis=(1, 2))  # Mean squared error for each sample\n",
    "    return errors\n",
    "\n",
    "# Calculate reconstruction error on validation data\n",
    "validation_errors = calculate_reconstruction_error(fraud_detector, X_val)\n",
    "\n",
    "# Set a threshold based on the reconstruction error distribution of valid data\n",
    "threshold = np.percentile(validation_errors, 95)  # 95th percentile of validation error as threshold\n",
    "\n",
    "# Function to detect fraud based on reconstruction error\n",
    "def detect_fraud(model, transaction_data, threshold):\n",
    "    error = calculate_reconstruction_error(model, transaction_data)\n",
    "    fraud_flags = error > threshold\n",
    "    return fraud_flags, error\n",
    "\n",
    "# Use the function to detect fraud\n",
    "fraud_flags, transaction_errors = detect_fraud(autoencoder, new_transactions, threshold)\n",
    "\n",
    "# `fraud_flags` will be True for transactions likely to be fraud\n",
    "# `transaction_errors` gives the reconstruction error for each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Load user-specific data\n",
    "# Assuming you have a function to prepare user-specific data\n",
    "user = combined_data[combined_data['user']=='User1']\n",
    "features = user[['xpos', 'ypos', 'key_encoded_1', 'key_encoded_2', 'event', 'os', 'browser', 'DU.key1.key1', 'DD.key1.key2', 'DU.key1.key2', 'UD.key1.key2', 'UU.key1.key2']].values\n",
    "X_user, y_user = create_sequences(features, SEQ_LENGTH)  # User-specific dataset\n",
    "\n",
    "# Load the pre-trained model\n",
    "#pretrained_model = load_model('pretrained_lstm_model.h5')\n",
    "\n",
    "# Fine-tune the model on user-specific data\n",
    "# Optionally, you can freeze some layers if the dataset is very small\n",
    "for layer in model.layers[:-2]:  # Freeze all layers except the last two\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model again after modifying the layers\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Fine-tune the model\n",
    "history_user = model.fit(X_user, X_user, epochs=5, batch_size=8, validation_split=0.2)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "#pretrained_model.save('fine_tuned_model_user.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
